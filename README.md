# Gated RNN

ì‘ì„±ì¼: April 23, 2023
ì‘ì„±í•œ ì‚¬ëŒ: ì–‘ì›í¬
íƒœê·¸: AI

RNN: Recurrent Neural Network for sequences

í˜„ì¬ ì‹œì ì˜ inputê³¼ ì´ì „ì˜ hidden stateë“¤ì„ í•©ì³ ìµœì¢… inputì„ ë§Œë“¦.

ğŸŒŸÂ í˜„ì¬ ì‹œì ì˜ inputê³¼ ë°”ë¡œ ì´ì „ë§Œì˜ inputì„ í•©ì³ì„œ inputì„ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹˜.

ë§¤ timestampë§ˆë‹¤ ì´ì „ì˜ ì •ë³´ + ìƒˆë¡œìš´ input ì •ë³´.

ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì·¨ì‚¬ ì„ íƒí•˜ë©°, hidden layerëŠ” inputê³¼ output ì‚¬ì´ì˜ barrier ì—­í• ì„ í•¨. outputì€ ë„¤íŠ¸ì›Œí¬ì— ì˜í•´ì„œ ê²°ì •ë¨. ë§Œì•½ ì¤‘ê°„ layerì— inputì´ ë“¤ì–´ê°€ì§€ ì•Šë”ë¼ë„ hidden layerëŠ” ë§¤ timestampë§ˆë‹¤ ë°”ë€œ. 

backpropagationì—ì„œëŠ” ì¼ë°˜ì ì¸ NNê³¼ ë‹¬ë¦¬ weight, synapse 0, 1, hë¥¼ ë°˜ë³µí•´ì„œ ì‚¬ìš©í•¨.

$$
h_t = f(h_{t-1}, x_t), 
h_t = g(Wx_t + UH_{t-1})
$$

ì—¬ê¸°ì„œ fëŠ” RNNì˜ í•¨ìˆ˜.

gí•¨ìˆ˜ëŠ” smooth, bounded function. (logistic sigmoid, tanh function)

### RNNì˜ ë‹¨ì  ë° í•´ê²°ì±….

- back propagationí•  ë•Œì˜ Gradient vanishing problem. Long-term dependencyë¥¼ captureí•˜ëŠ” ê²ƒì´ ì–´ë ¤ì›€.
- í•´ê²°ì±…: more sophisticated activation function. (Gated Recurrent Neural network)
- Gate: ì„ íƒì ìœ¼ë¡œ ì •ë³´ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì œì™¸ì‹œí‚¤ê²Œ í•˜ëŠ” ì¥ì¹˜.
- Ex. LSTM, GRU

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-04-23 18.11.01.png](README/image1.png)

cell state: ì •ë³´ ì „ë‹¬ highway, ê²Œì´íŠ¸ë“¤ì— ì˜í•´ ì–´ë–¤ ì •ë³´ê°€ í—ˆìš©ë˜ëŠ”ì§€ ê²°ì •ë¨.

forget gate: ì •ë³´ë¥¼ keep / forgetí• ì§€ ê²°ì •.

input gate: ìƒˆë¡œìš´ cell valueë¥¼ ìœ„í•´ ê°€í•´ì§€ëŠ” inputì„ ì¡°ì ˆ.

output gate: ë‹¤ìŒ hidden state(memory)ë¥¼ ê²°ì •.

![á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2023-04-23 18.17.07.png](README/image2.png)

<aside>
ğŸ’¡ ì›ë¬¸ ë° ìœ íŠœë¸Œ ë§í¬: [https://www.youtube.com/watch?v=5Ar1aN9gceg](https://www.youtube.com/watch?v=5Ar1aN9gceg)

[[paper]LSTM_GRU.pdf](README/paperLSTM_GRU.pdf)

</aside>